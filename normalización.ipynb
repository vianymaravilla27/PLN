{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08391279",
   "metadata": {},
   "source": [
    "# NORMALIZACIÓN DE TEXTO EXCELSIOR\n",
    "# PRE\n",
    "# VIANEY MARAVILLA PÉREZ \n",
    "# PROCESAMIENTO DE LENGUAJE NATURAL\n",
    "# 6AM1\n",
    "# 01 DE SEPTIEMBRE DE 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e07bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias...\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c2b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(path = './../Texto/'):\n",
    "    # Obtenemos el corpus del directorio...\n",
    "    corpus = PlaintextCorpusReader(path, '.*')\n",
    "    file_list = corpus.fileids()\n",
    "    # Juntamos todo el texto de todos los archivos...\n",
    "    all_text = ''\n",
    "    for file in file_list:\n",
    "        with open(path + file, encoding = 'utf-8') as rfile:\n",
    "            text = rfile.read()\n",
    "            all_text += text\n",
    "    # Removemos las etiquetas html...\n",
    "    soup = BeautifulSoup(all_text, 'lxml')\n",
    "    clean_text = soup.get_text()\n",
    "    clean_text = clean_text.lower()\n",
    "    # Tokenizamos el texto...\n",
    "    words = clean_text.split()\n",
    "    alphabetic_words = []\n",
    "    for word in words:\n",
    "        token = []\n",
    "        for character in word:\n",
    "            if re.match(r'^[a-záéíóúñü+$]', character):\n",
    "                token.append(character)\n",
    "        token = ''.join(token)\n",
    "        if token != '':\n",
    "            alphabetic_words.append(token)\n",
    "    # Quitamos las Stop words...\n",
    "    with open('./stopwords_es.txt', encoding = 'utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [w.strip() for w in stop_words]\n",
    "    final_words = [word for word in alphabetic_words if word not in stop_words]\n",
    "    print('Esta es la normalización realizada...')\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8ada26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta es la normalización realizada...\n",
      "Algunas palabras despues de la normalización: \n",
      "['emodhtm', 'httpwwwexcelsiorcommxarthtml', 'excelsior', 'editorial', 'martes', 'abril', 'monstruosa', 'diferencia', 'colosistas', 'colosismo', 'luis', 'gutierrez', 'gonzalez', 'luis', 'gutiérrez', 'sotomayor', 'federico', 'arreola', 'colosistas', 'cabales', 'según', 'dijo', 'amigo', 'luis', 'donaldo', 'ciertamente', 'nombre', 'circunstancias', 'luis', 'donaldo', 'colosio', 'llenado', 'insistentemente', 'volúmenes', 'espacios', 'medios', 'comunicación', 'renovada', 'actualidad', 'padecido', 'frenético', 'vaivén', 'ficciones', 'judiciales', 'políticas', 'integran', 'disgregan', 'metafísicas', 'metafísicas', 'aún', 'luis', 'donaldo', 'desprende', 'envuelve', 'lado', 'espejo', 'dos', 'años', 'eternos', 'insolvencias', 'dale', 'dale', 'fantasía', 'magia', 'dónde', 'quedó', 'bolita', 'traído', 'pueblo', 'hastío', 'cansancio', 'inminencia', 'percibe', 'váyanse', 'diablo', 'quórum', 'nacional', 'veía', 'decidido', 'instalar', 'sécula', 'seculórum', 'demandas', 'justicia', 'segundo', 'aniversario', 'asesinato', 'colosismo', 'astroso', 'luto', 'protagónico', 'intentado', 'empapar', 'drama', 'colosista', 'espesas', 'negras', 'lágrimas', 'llorona', 'profesional', 'velorio', 'antigüita', 'diciéndose', 'heredero', 'ideas', 'derechos', 'supuesto', 'paradigma', 'trampa', 'tendió', 'esotérico', 'colosismo', 'presidente', 'zedillo', 'ponce', 'león', 'amigo', 'líder', 'mandatario', 'dedica', 'culto', 'seguimiento', 'espirituales', 'dándole', 'satisfacción', 'compromisos', 'políticos', 'aquel', 'contrajo', 'actitud', 'moral', 'propició', 'trampa', 'fulano', 'zutano', 'dijeron', 'discípulos', 'hermanos', 'compañeros', 'lucha', 'asesores', 'caído', 'veras', 'hicieron', 'lado', 'vieron', 'cargar', 'don', 'ernesto', 'tropel', 'búfalos', 'llora', 'mama', 'colosistas', 'supieron', 'quisieron', 'vergüenza', 'ser', 'confundidos', 'pegarse', 'ubre', 'mal', 'hicieron', 'colosismo', 'repartió', 'mercedes', 'presidenciales', 'parteaguas', 'brecha', 'insondable', 'separando', 'colosistas', 'colosismo', 'bandos', 'luis', 'donaldo', 'llamaba', 'dijo', 'nombres', 'respectivos', 'cabales', 'oportunistas', 'efeméride', 'dolorosa', 'colosismo', 'intentó', 'constituirse', 'dogma', 'doctrina', 'credo', 'político', 'allá', 'magdalena', 'leyendo', 'lumpen', 'histriónico', 'viendo', 'protagonismos', 'don', 'luis']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    words = normalize()\n",
    "    print(f'Algunas palabras despues de la normalización: \\n{words[:200]}')\n",
    "except Exception as e:\n",
    "    print('Ocurrió el siguiente error: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a9fc3",
   "metadata": {},
   "source": [
    "# SEGUNDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5cc57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias...\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a70ea6a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:33\u001b[1;36m\u001b[0m\n\u001b[1;33m    lemmatized_text=[]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def normalize(path = './../Texto/'):\n",
    "    # Obtenemos el corpus del directorio...\n",
    "    corpus = PlaintextCorpusReader(path, '.*')\n",
    "    file_list = corpus.fileids()\n",
    "    # Juntamos todo el texto de todos los archivos...\n",
    "    all_text = ''\n",
    "    for file in file_list:\n",
    "        with open(path + file, encoding = 'utf-8') as rfile:\n",
    "            text = rfile.read()\n",
    "            all_text += text\n",
    "    # Removemos las etiquetas html...\n",
    "    soup = BeautifulSoup(all_text, 'lxml')\n",
    "    clean_text = soup.get_text()\n",
    "    clean_text = clean_text.lower()\n",
    "    # Tokenizamos el texto...\n",
    "    words = clean_text.split()\n",
    "    alphabetic_words = []\n",
    "    for word in words:\n",
    "        token = []\n",
    "        for character in word:\n",
    "            if re.match(r'^[a-záéíóúñü+$]', character):\n",
    "                token.append(character)\n",
    "        token = ''.join(token)\n",
    "        if token != '':\n",
    "            alphabetic_words.append(token)\n",
    "    # Quitamos las Stop words...\n",
    "    with open('./Stopwords_es_NLTK.txt', encoding = 'utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [w.strip() for w in stop_words]\n",
    "    final_words = [word for word in alphabetic_words if word not in stop_words]\n",
    "    print('Fin de la normalización...')\n",
    "    return final_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a560cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de la normalización...\n",
      "Algunas palabras despues de la normalización: \n",
      "['emodhtm', 'httpwwwexcelsiorcommxarthtml', 'excelsior', 'editorial', 'martes', 'abril', 'monstruosa', 'diferencia', 'colosistas', 'colosismo', 'luis', 'gutierrez', 'gonzalez', 'luis', 'gutiérrez', 'sotomayor', 'federico', 'arreola', 'colosistas', 'cabales', 'según', 'dijo', 'amigo', 'luis', 'donaldo', 'ciertamente', 'nombre', 'circunstancias', 'luis', 'donaldo', 'colosio', 'llenado', 'insistentemente', 'volúmenes', 'espacios', 'medios', 'comunicación', 'renovada', 'actualidad', 'padecido', 'frenético', 'vaivén', 'ficciones', 'judiciales', 'políticas', 'integran', 'disgregan', 'metafísicas', 'metafísicas', 'aún', 'luis', 'donaldo', 'desprende', 'envuelve', 'lado', 'espejo', 'dos', 'años', 'eternos', 'insolvencias', 'dale', 'dale', 'fantasía', 'magia', 'dónde', 'quedó', 'bolita', 'traído', 'pueblo', 'hastío', 'cansancio', 'inminencia', 'percibe', 'váyanse', 'diablo', 'quórum', 'nacional', 'veía', 'decidido', 'instalar', 'sécula', 'seculórum', 'demandas', 'justicia', 'segundo', 'aniversario', 'asesinato', 'colosismo', 'astroso', 'luto', 'protagónico', 'intentado', 'empapar', 'drama', 'colosista', 'espesas', 'negras', 'lágrimas', 'llorona', 'profesional', 'velorio', 'antigüita', 'diciéndose', 'heredero', 'ideas', 'derechos', 'supuesto', 'paradigma', 'trampa', 'tendió', 'esotérico', 'colosismo', 'presidente', 'zedillo', 'ponce', 'león', 'amigo', 'líder', 'mandatario', 'dedica', 'culto', 'seguimiento', 'espirituales', 'dándole', 'satisfacción', 'compromisos', 'políticos', 'aquel', 'contrajo', 'actitud', 'moral', 'propició', 'trampa', 'fulano', 'zutano', 'dijeron', 'discípulos', 'hermanos', 'compañeros', 'lucha', 'asesores', 'caído', 'sido', 'veras', 'hicieron', 'lado', 'vieron', 'cargar', 'don', 'ernesto', 'tropel', 'búfalos', 'llora', 'mama', 'colosistas', 'supieron', 'quisieron', 'vergüenza', 'ser', 'confundidos', 'pegarse', 'ubre', 'mal', 'hicieron', 'colosismo', 'repartió', 'mercedes', 'presidenciales', 'parteaguas', 'brecha', 'insondable', 'separando', 'colosistas', 'colosismo', 'bandos', 'luis', 'donaldo', 'llamaba', 'dijo', 'nombres', 'respectivos', 'cabales', 'oportunistas', 'efeméride', 'dolorosa', 'colosismo', 'intentó', 'constituirse', 'dogma', 'doctrina', 'credo', 'político', 'allá', 'magdalena', 'leyendo', 'lumpen', 'histriónico', 'viendo', 'protagonismos', 'don']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    words = normalize()\n",
    "    print(f'Algunas palabras despues de la normalización: \\n{words[:200]}')\n",
    "except Exception as e:\n",
    "    print('Ocurrió el siguiente error: ', e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c1b7d47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m lemmatized_text\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfinal_words\u001b[49m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m lemmas\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      5\u001b[0m         lemmatized_text\u001b[38;5;241m.\u001b[39mappend(lemmas[word])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_words' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatized_text=[]\n",
    "\n",
    "for word in final_words:\n",
    "    if word in lemmas.keys():\n",
    "        lemmatized_text.append(lemmas[word])\n",
    "    else:\n",
    "        lemmatized_text.append(word)\n",
    "print(lemmatized_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "899b5e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lemma_dict (fname):\n",
    "    from pickle import dump\n",
    "    \n",
    "    with open(fname, encoding = 'latin-1') as f :\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        lemmas={}\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                words = line.split()\n",
    "                token = words[0].strip()\n",
    "                token = token.replace('8','')\n",
    "                lemma = words [-1].strip()\n",
    "                lemmas[token] = lemma \n",
    "                print(lemmas_list = list(lemmas.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9fb4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_es= stopwords.words('spanish')\n",
    "stopwords_es= list(stopwords_es)\n",
    "\n",
    "with open ('Stopwords_es_NLTK.txt','w', encoding = 'utf-8') as f:\n",
    "    for w in stopwords_es:\n",
    "        f.write (w + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b9cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
